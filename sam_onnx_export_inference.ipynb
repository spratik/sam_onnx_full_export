{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spratik/sam_onnx_full_export/blob/main/sam_onnx_export_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": true
        },
        "id": "XJS1rSmGEe_j",
        "outputId": "75d81775-3a20-4090-f7e9-57aea4ca643b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-zzk4u150\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-zzk4u150\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment-anything\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36587 sha256=82d372123d090371acf130623a49e7d7ee650179decf35cd1d393204469f7f94\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1wsxqtmd/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment-anything\n",
            "Installing collected packages: segment-anything\n",
            "Successfully installed segment-anything-1.0\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.15.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install onnx\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUqqvVJ3EfdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": [
        "from segment_anything import sam_model_registry\n",
        "from segment_anything.utils.onnx import SamOnnxModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "F3dFX7CtEe_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=b1146c74a453313e1d4f23d660799fc763628b5719a8c66f7a493f0db545990e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "\n",
            "Saved under sam_vit_b_01ec64.pth\n"
          ]
        }
      ],
      "source": [
        "# Download SAM model checkpoint\n",
        "!pip install wget\n",
        "!python -m wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "ZtSMws3JEe_l",
        "outputId": "577181cf-ebd8-4716-abef-fa1e3d30a445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "# Load SAM model\n",
        "sam = sam_model_registry[\"vit_b\"](checkpoint=\"./sam_vit_b_01ec64.pth\")"
      ],
      "metadata": {
        "id": "YOrBXtFFEe_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:258: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_h > 0 or pad_w > 0:\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:306: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if rel_pos.shape[0] != max_rel_dist:\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:320: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/image_encoder.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if Hp > H or Wp > W:\n"
          ]
        }
      ],
      "source": [
        "# Export images encoder from SAM model to ONNX\n",
        "torch.onnx.export(\n",
        "    f=\"vit_b_encoder.onnx\",\n",
        "    model=sam.image_encoder,\n",
        "    args=torch.randn(1, 3, 1024, 1024),\n",
        "    input_names=[\"images\"],\n",
        "    output_names=[\"embeddings\"],\n",
        "    export_params=True\n",
        ")"
      ],
      "metadata": {
        "id": "jusePKa3Ee_l",
        "outputId": "42491fe1-ae69-4abd-afb8-77d781a65748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/segment_anything/modeling/transformer.py:232: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  attn = attn / math.sqrt(c_per_head)\n",
            "/usr/local/lib/python3.10/dist-packages/segment_anything/utils/onnx.py:97: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  score_reweight = torch.tensor(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5856: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Export mask decoder from SAM model to ONNX\n",
        "onnx_model = SamOnnxModel(sam, return_single_mask=True)\n",
        "embed_dim = sam.prompt_encoder.embed_dim\n",
        "embed_size = sam.prompt_encoder.image_embedding_size\n",
        "mask_input_size = [4 * x for x in embed_size]\n",
        "dummy_inputs = {\n",
        "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
        "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
        "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
        "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
        "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
        "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
        "}\n",
        "output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
        "torch.onnx.export(\n",
        "    f=\"vit_b_decoder.onnx\",\n",
        "    model=onnx_model,\n",
        "    args=tuple(dummy_inputs.values()),\n",
        "    input_names=list(dummy_inputs.keys()),\n",
        "    output_names=output_names,\n",
        "    dynamic_axes={\n",
        "        \"point_coords\": {1: \"num_points\"},\n",
        "        \"point_labels\": {1: \"num_points\"}\n",
        "    },\n",
        "    export_params=True,\n",
        "    opset_version=17,\n",
        "    do_constant_folding=True\n",
        ")"
      ],
      "metadata": {
        "id": "l_zvuuWHEe_l",
        "outputId": "dd342d79-50fe-4d21-dc37-eec4927c5ae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "waLsjvzCFZrK",
        "outputId": "cbba21a0-b49e-4fe9-8cc9-c1db664a777a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  sam_vit_b_01ec64.pth  vit_b_decoder.onnx  vit_b_encoder.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime\n",
        "!pip install Pillow\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "dACtPTnmFnTo",
        "outputId": "58a9acfd-bad4-4bc2-87e9-b4ddf1088008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.16.3\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "AkP7KtYyFr0s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD IMAGE\n",
        "img = Image.open(\"cat_dog.jpg\").convert(\"RGB\")\n",
        "img.size"
      ],
      "metadata": {
        "id": "OnJH16zlFuO5",
        "outputId": "3bf1f6d1-d0a1-4ec4-d4b4-f32107d695ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(612, 415)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orig_width, orig_height = img.size\n",
        "resized_width, resized_height = img.size\n",
        "\n",
        "if orig_width > orig_height:\n",
        "    resized_width = 1024\n",
        "    resized_height = int(1024 / orig_width * orig_height)\n",
        "else:\n",
        "    resized_height = 1024\n",
        "    resized_width = int(1024 / orig_height * orig_width)\n",
        "\n",
        "img = img.resize((resized_width, resized_height), Image.Resampling.BILINEAR)\n",
        "\n",
        "img.size"
      ],
      "metadata": {
        "id": "2YYEBGH9GGCb",
        "outputId": "e1aba6a5-11d2-43db-a3f1-8ca0314763d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1024, 694)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make image square 1024x1024 by padding short side by zeros\n",
        "if resized_height < resized_width:\n",
        "    input_tensor = np.pad(input_tensor,((0,0),(0,0),(0,1024-resized_height),(0,0)))\n",
        "else:\n",
        "    input_tensor = np.pad(input_tensor,((0,0),(0,0),(0,0),(0,1024-resized_width)))\n",
        "\n",
        "input_tensor.shape"
      ],
      "metadata": {
        "id": "fQlILOp5GQ4Z",
        "outputId": "95e2db24-033a-4208-b8fd-718237e4cae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 1024, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. GET IMAGE EMBEDDINGS USING IMAGE ENCODER\n",
        "encoder = ort.InferenceSession(\"vit_b_encoder.onnx\")\n",
        "outputs = encoder.run(None,{\"images\":input_tensor})\n",
        "embeddings = outputs[0]\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "vGdfXRiHGV_5",
        "outputId": "4771e51a-4733-460c-e0d3-8b4ec7b503d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 256, 64, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODE PROMPT (single point)\n",
        "input_point = np.array([[321,230]])\n",
        "input_label = np.array([1])\n",
        "\n",
        "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
        "onnx_label = np.concatenate([input_label, np.array([-1])])[None, :].astype(np.float32)\n",
        "\n",
        "coords = deepcopy(onnx_coord).astype(float)\n",
        "coords[..., 0] = coords[..., 0] * (resized_width / orig_width)\n",
        "coords[..., 1] = coords[..., 1] * (resized_height / orig_height)\n",
        "\n",
        "onnx_coord = coords.astype(\"float32\")\n",
        "onnx_coord"
      ],
      "metadata": {
        "id": "kIgYZyw2HuvA",
        "outputId": "dbf34c3b-cc75-47ae-ee30-e534e86c71fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[537.098 , 384.6265],\n",
              "        [  0.    ,   0.    ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN DECODER TO GET MASK\n",
        "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
        "onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
        "\n",
        "decoder = ort.InferenceSession(\"vit_b_decoder.onnx\")\n",
        "masks,_,_ = decoder.run(None,{\n",
        "    \"image_embeddings\": embeddings,\n",
        "    \"point_coords\": onnx_coord,\n",
        "    \"point_labels\": onnx_label,\n",
        "    \"mask_input\": onnx_mask_input,\n",
        "    \"has_mask_input\": onnx_has_mask_input,\n",
        "    \"orig_im_size\": np.array([orig_height, orig_width], dtype=np.float32)\n",
        "})"
      ],
      "metadata": {
        "id": "sAVkTUC0Ja2-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POSTPROCESS MASK\n",
        "mask = masks[0][0]\n",
        "mask = (mask > 0).astype('uint8')*255\n",
        "mask.shape"
      ],
      "metadata": {
        "id": "LWEV7DKZJlWH",
        "outputId": "cebdaf58-13bb-4342-a1d7-c799745cb739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(415, 612)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_mask = Image.fromarray(mask,\"L\")\n",
        "img_mask"
      ],
      "metadata": {
        "id": "Y92Y34gYK251",
        "outputId": "d836fa6d-5c72-47ba-972e-356879f6d7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=612x415>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAGfCAAAAADUx5EnAAAFg0lEQVR4nO3d3ZLTOBSF0WSK939lczHQtH/bSbxtHWutG6aLDCjWV0dOaMLjAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHBXz6sX0K7hz48u0adcwTXDv/989yINH/y/d+IaLBu+f/HWRfr6FVxiV2DRMPn65cv0eaQ38t/VC6hhGt3Rj783kS35tJFh88vuiGyflzqZPbjzykS2YKmJzjv5iMj22l+ZHidENheIpO/uRLbb3lD6DmqJyPbbV4/GZkR2sJXGuk5PZDPrPXRdygdERpzIjmXYLRDZKyT0FpERJ7KX/DTKjLolIiNOZK/ZHlUG2SKRHWijsa6/OVZkM9s9bIRkjq0QGXEiI+7X1QsoZ1g7UJ2Wa0yyNwy+P/slInvLPLPtxrp+cem4fNfoIwhMsU0i+4C29nFcEicy4kRGnMiIExlxIpvxmvFoIiNOZMSJjDiREScy4kRGnMhm+v62nASREScy4kRGnMiIExlxIiNOZMSJjDiREScy4kRGnMiIExlxIiNOZDP+ttLRREacyKYMssOJjDiREScy4kR2ir5v9ERGnMiIE9k5uj4vRUacyE7S8ygTGXEim0p9FEbHo0xkUx3HkCIy4kRGnMiIExlxIptw3388kZ2m33xFRpzIiBMZcSIjTmTEiWys35eAQSI7Tb//0onIiBPZafo9iUVGnMiIExlxIiNOZGPJ9xm6vfMXGXEiO1Gvo0xkxImMOJGdqdPzUmTEiWwi+70SfY4ykREnsnN1OcpERpzITtbjKBPZ2TqsTGSn66+yfv92w4pTEujsqptkVxj6mmYiu0ZXlYlsrKvNP4vILtJTzSIjTmQjPc2X84jsKh31/OvqBbDmq8Ly76qJrEnD/IvCqYmsOSvn6FC3srorjzj1Run54m9Ydq/KLjyj8bvxortVdNkhjTf2KLpf3sKopeQfrYusmoKViYw4kZVTb5SJrJ5y92Ui+6bM5pVZ6P9ERpzI/ik0Hwot9SGyqkpVVvId5IxS+/aotHUmGXEi+6vaICu0YJERJ7I/6syFL2WWLDLiRFZYlVEmMuJEVlmRUSay0mpUJjLiRPZHnT+kGSkxykRGnMiKqzDKREacyP4qelNWgciIE9mXoqOswE2ZyMprvzKR/VN0lLVfmci+qVpZ60R2A62PMpF9V3WUNV6ZyEaqVtY2kY29VNnzKco9XKWJ3SfP88XHZzW9jybZxN7d+vs4w+xnLtHcnuE0vm7Xj7Om97HpxV1lqZnxh+7PL9vFnTW9j47LBWs79tx4wMXH5vWzdIPIlvwQzPJPuztbI7JFK708H4+tmFS2zHVZMzmA9l2oC0+thney4aU1Yng8nvv/ibbrKmt4JxteWkkiW+Ce7Fju/heI7GhXVdbwmxgiO5xZNiUy4kR2PKNsQmQBKhsTWYLKRkR2G+2+vBRZhFH2nciIE9lttDs9RRbR7v3RFURGnMgSDLIRkREnMuJElnDFC712X1yK7C4abkxkN9FyYyIjT2TEiewWmj4tRRbhzdgRkREnsoS2T6/TiSzBcTkiMuJEdgeNH88iS2h8088msohYZSU/0KXimks4/N7/a6dmv3Lze9j8Aqs6MrLJJr31GZBXan+FVR1U2dIGDT8+oi3uyVIO2fsdt2DtN1ZhiXV9Osy2N2cos3tFllnVR5ndZm9u80Ra91JuN9uVmz2dpu3s7H5bcr9n1LifSrvjhtzxOTVuI7Ob7sZNn1bzFkq771bc95k1b9SZfQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKBbvwFeupRl+ABNpAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}